{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be592340-2406-494f-8f68-515385c30ec0",
   "metadata": {},
   "source": [
    "# Probabilistic Programming for Music Research\n",
    "\n",
    "This notebook shows a case study of how to use Bayesian models and probabilistic programming to make inferences about musical data and corpora.\n",
    "We will focus on the python library [PyMC](https://www.pymc.io/welcome.html), but also show some examples in [pyro](http://pyro.ai/) and [numpyro](https://num.pyro.ai/en/stable/).\n",
    "\n",
    "The case study deals with a toy example: modeling the sizes of melodic intervals in Bach chorales.\n",
    "You will see how to express different assumptions about how these intervals work in probabilistic models,\n",
    "how to apply these models to a dataset to make inferences,\n",
    "and how to compare and evaluate the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f702510-2aad-4022-9ad5-67f2acaa05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyMC (good for MCMC inference)\n",
    "import pymc as pm\n",
    "import pytensor as ptn\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442c59b-1c54-4f5b-a0a8-788099279117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "rng = np.random.default_rng(14842)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9915c10-aefc-43f5-9c97-c307f9be21f5",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Here we just load the dataset that we will work with.\n",
    "It contains a list so-called \"bigrams\" - pairs of sequential notes - from each voice of the first 100 chorales in the 371 Bach chorales collection.\n",
    "Thus, for each bigram we know the pitch of the first note (in MIDI as well as tpc (= line-of-fifths position) + octave),\n",
    "the pitch of the second note, the voice (staff) and piece in which it appears,\n",
    "as well as some derived information such as the interval in semitones and the absolute size of the interval in semitones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad6285-eeeb-442c-bd1e-b3667baff797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dists = pd.read_csv(\"bigrams.tsv\", sep=\"\\t\", index_col=[\"piece\", \"staff\", \"bigram_id\"])\n",
    "dists = pd.read_csv(\"https://github.com/Amsterdam-Music-Lab/gmth23-bayes-workshop/raw/main/bigrams.tsv\", sep=\"\\t\", index_col=[\"piece\", \"staff\", \"bigram_id\"])\n",
    "dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b243d-f22f-4bbe-9328-cfc05610d48b",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution of the (directed) intervals in each voice.\n",
    "We can see that they are usually centered around 0 (unison) and roughly symmetric.\n",
    "Smaller intervals are more frequent than larger intervals.\n",
    "The distribution looks similar in the different voices, but not exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4c164-03b5-4679-b852-8f437d2c5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(dists, x=\"int_semitones\")\n",
    "        .facet(\"staff\")\n",
    "        .add(so.Bars(alpha=0.5), so.Hist(\"probability\", discrete=True))\n",
    "        #.add(so.Line(), so.KDE())\n",
    "        .layout(size=(13,4))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a4b64-00c2-4724-b9c3-6a60352afde2",
   "metadata": {},
   "source": [
    "Here is the same plot for the absolute distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b775365-02a5-42fe-bd1e-43b7c29e1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(dists, x=\"dist_semitones\")\n",
    "        .facet(\"staff\")\n",
    "        .add(so.Bars(alpha=0.5), so.Hist(\"probability\", discrete=True))\n",
    "        #.add(so.Line(), so.KDE())\n",
    "        .layout(size=(13,4))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d85f12-bcc5-43ed-9a15-533d8f4999b4",
   "metadata": {},
   "source": [
    "Let's now extract some of this data that we will use for modelling:\n",
    "- the observed absolute intervals or \"distances\"; this is what we want to understand\n",
    "- the staff in which a bigram appears\n",
    "- the pitch (MIDI) of the first note in each bigram\n",
    "\n",
    "The latter two might have an influence on the intervals that we observe,\n",
    "and this relationship is what we are going to model and make inferences about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457e156-19cb-4b81-b868-cb76b16cc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.array(dists[\"dist_semitones\"].to_numpy())\n",
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99fe05-3745-4034-b0d6-2b9b350ca2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = dists.index.get_level_values(\"staff\").to_numpy()-1\n",
    "voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ab09e-6825-45f7-8108-aadef58f880f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p0 = dists[\"n0_midi\"].to_numpy()\n",
    "p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa357733-e75c-49d3-a8aa-e9fce0bad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function for later\n",
    "def estimate_beta(samples):\n",
    "    mean = samples.mean(dim=[\"chain\", \"draw\"]).to_numpy()\n",
    "    var = samples.var(dim=[\"chain\", \"draw\"]).to_numpy()\n",
    "    common = (mean * (1-mean) / var) - 1\n",
    "    alpha = common * mean\n",
    "    beta = common * (1-mean)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c5c9b-81f9-4017-bfb4-5df3719ee7d9",
   "metadata": {},
   "source": [
    "# Model 1: one global spread parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26633574-0992-4b3c-82f1-09dd081b464d",
   "metadata": {},
   "source": [
    "Our first model is very simple: we just assume the observed intervals to be random.\n",
    "But \"random\" doesn't mean \"completely arbitrary\",\n",
    "we still assume that they follow a certain distribution.\n",
    "If we look at the shape of the data in the plots above, we can see that\n",
    "very large intervals are possible but small steps or unisons are most likely.\n",
    "Thus, we pick a [geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution),\n",
    "which works as follows:\n",
    "You flip a coin (with a certain probability $\\theta$) until it shows *heads* for the first time.\n",
    "You then count the number of trials.\n",
    "This is a bit like taking a number of steps (here: steps away from the previous note),\n",
    "but at each step you flip a coin to determine if you want to keep walking or stay where you are.\n",
    "So it's not an entirely unreasonable model for musical intervals,\n",
    "we just try to decide how far we want to go away from the previous note.\n",
    "\n",
    "If we write this (very simplistic) view on how melodic intervals work\n",
    "and try to express it as a generative model, it looks something like this:\n",
    "- Choose a $\\theta$ between 0 and 1\n",
    "- For each $i$ between $0$ and $N$:\n",
    "  - Choose $x_i$ from a geometric distribution with parameter $\\theta$.\n",
    "\n",
    "Since our model needs a prior distribution for $\\theta$,\n",
    "we will choose a [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution).\n",
    "The reason for this is that the beta distribution is a so-called \"conjugate prior\" to the geometric distribution.\n",
    "That means: if the prior $p(\\theta)$ is beta-distributed and it is used in a geometric distribution ($p(x|\\theta)$),\n",
    "then the posterior $p(\\theta|x)$ will again be a beta distribution!\n",
    "In particular, we choose the distribution $Beta(0.5, 0.5)$, which is the so-called [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior), an uninformative prior.\n",
    "(In our case, we have so much data that the choice of the prior doesn't really make a big difference for $\\theta$,\n",
    "so we could also pick $Beta(1,1)$, which is actually a uniform distribution between 0 and 1).\n",
    "\n",
    "If we now write down our model, it looks like this:\n",
    "- Choose $\\theta \\sim Beta(0.5, 0.5)$\n",
    "- For each $0 \\leq i \\leq N$:\n",
    "  - Choose $x_i \\sim Geometric_0(\\theta)$.\n",
    " \n",
    "And that is exactly how we define the model in PyMC!\n",
    "The main differences are:\n",
    "- We give a name to each random variable *twice*:\n",
    "  in every distribution, so that PyMC can keep track of the variable during sampling (e.g. `pm.Beta(\"theta_global\", ...)`);\n",
    "  and as a python variable in our program so that we can use the value later on in the model (e.g. `theta = ...`),\n",
    "- We already say that the $x_i$ are *observed* and provide the corresponding values (`observed=...`).\n",
    "- We tell PyMC to draw 10.000 samples from the posterior distribution $p(\\theta | \\vec{x})$ using `pm.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5001d-3197-4ed7-85b7-cda16a88abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_simple:\n",
    "    theta = pm.Beta(\"theta_global\", 0.5, 0.5)\n",
    "    obs = pm.Geometric(\"obs\", p=theta, observed=observations+1) # +1 for geometric distribution (starts at 1, not 0)\n",
    "\n",
    "    idata_simple = pm.sample(5_000, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9114b59-3996-4199-9e00-44976b433221",
   "metadata": {},
   "source": [
    "The returned samples are stored in an \"inference data\" object,\n",
    "together with some extra informtation (e.g. the numer of chains and samples, infos about the sampling process, etc.).\n",
    "It is rendered in a nice way in the notebook.\n",
    "If you look inside the \"posterior\" branch, you can find the sampled variables there (here just `\"theta_global\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9052e4-7474-4020-bb56-4fd742f8ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbccdf2-9bdb-4a94-a675-40b374641e45",
   "metadata": {},
   "source": [
    "We can have a look at the samples that were drawn using `arviz` plotting functions.\n",
    "The left plot shows two lines because we have used two MCMC chains.\n",
    "The right plot shows which values were drawn over time, so we can see if there are systematic differences\n",
    "between early and late samples (that would be bad!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731d930-60a9-48db-882a-d6b49d4071ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata_simple);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ba425-5f6b-41d7-9bd6-e6c2c0d9cd29",
   "metadata": {},
   "source": [
    "This is another view of the posterior samples with some extra information.\n",
    "As you can see, the \"true\" value of `theta` is probably close to 0.316 or 0.317,\n",
    "but there is always a bit of uncertainty left (although it's not very large here).\n",
    "We can obtain the precise expected value by just taking the mean of the samples,\n",
    "but keep in mind that this is still only an estimate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83efa0-987d-4b5a-8c0f-3f65c62b36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(idata_simple);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f63986-d2ff-45a1-8e1e-e5942ae70472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected value:\n",
    "idata_simple.posterior[\"theta_global\"].to_numpy().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cab772-bba3-4574-9a72-cd771a272d33",
   "metadata": {},
   "source": [
    "# Model 2: one parameter per voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff462ff3-5938-41e6-b3f8-f934d9969054",
   "metadata": {},
   "source": [
    "The first model makes a lot of unrealistic assumptions about how interval sizes are determined.\n",
    "Basically, the size of an interval is completely independent from where it occurs, in which voice,\n",
    "or what happens around it, and all intervals follow the same distribution.\n",
    "\n",
    "Let's try to make the model a bit less unrealistic.\n",
    "What if the voice actually makes a difference?\n",
    "Think of a model that would implement this idea and then implement the model below.\n",
    "You can start with the simple model above and make some changes to it.\n",
    "\n",
    "Here is a hint that might be useful:\n",
    "The array `voice` above contains the voice of each note as a number between 0 and 3.\n",
    "You can use these numbers as an index into another array that contains information for voices.\n",
    "For example, if you want to translate the numbers into voice names,\n",
    "you could create a 4-elemen array with these voice names and then use the `voice` array\n",
    "to look up the voice name for each note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091720b-4c76-4493-9022-a67236397844",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_names = np.array([\"soprano\", \"alto\", \"tenor\", \"bass\"])\n",
    "voice_names[voice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f8526-9f89-40dd-8d6e-311134ee47d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d14a70-5fd7-4019-9589-b1662ef8c3d7",
   "metadata": {},
   "source": [
    "To get an idea of what happens in the model, plot the trace as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce477348-d0e2-4ae6-9872-0ed44757efe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c103a8ef-e439-4b5a-a650-afd9bc22fa19",
   "metadata": {},
   "source": [
    "Similarly, plot the posterior as before to see the results of inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa316e1-02d9-4646-84fa-602a7ec78d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02de146e-4319-431b-ab34-248605064a48",
   "metadata": {},
   "source": [
    "# Model 3: depend on register"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b3a5b-f4c9-4ba0-b582-ac7306cc769a",
   "metadata": {},
   "source": [
    "So what if the size of the intervals doesn't actually depend on the voice but on the register that they appear in?\n",
    "After all, vertical intervals tend to be larger in lower registers and smaller in higher registers too!\n",
    "Let's first have a look at the data, to see, if that could make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747b37a-be96-4c4a-81f6-b07047f8835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(dists, x=\"n0_midi\", y=\"int_semitones\").add(so.Dots())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e078da-2e5d-4a07-bd05-07b3fea07108",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(dists, x=\"n0_midi\", y=\"dist_semitones\").add(so.Dots()).add(so.Line(color=\"r\"), so.PolyFit(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2cb06-1106-448f-928a-3e158f6e70ad",
   "metadata": {},
   "source": [
    "In the second blot we see a slight tendency to get smaller intervals if the first note of the bigram is higher.\n",
    "The added regression line seems to support that.\n",
    "But is a linear regression really the right tool here?\n",
    "\n",
    "Let's instead try to modify our model.\n",
    "We keep the geometric distribution, but we make theta dependent on the pitch of the previous note ($p_0$).\n",
    "In particular, we use a linear dependency between the pitch and the output:\n",
    "\n",
    "$f(p_0) = a p_0 + b$\n",
    "\n",
    "Then, we wrap the resulting value into a sigmoid function,\n",
    "which is a curve with an S-shape that maps small values close to 0 and large value close to 1:\n",
    "\n",
    "$\\theta(p_0) = sigmoid(a p_0 + b) = \\dfrac{1}{1+exp(-(a p_0 + b))}$\n",
    "\n",
    "The parameter for the geometric distribution is then $\\theta(p_0)$ for the current note.\n",
    "\n",
    "Try to implement this model as below.\n",
    "You can find the preceding pitch of each note in the array `p0`.\n",
    "Think about the parameters of this models. What values can they take? What could be suitable priors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ad1f1-01f4-4f6f-bed2-5685a08224b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaeb1560-1c49-410c-90a2-326efbe5f7d8",
   "metadata": {},
   "source": [
    "Plot the trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac34dc-5c73-4f89-aee4-9f43ae74684d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84ec4a0a-1c37-46e4-a7aa-c7cf41fa12b6",
   "metadata": {},
   "source": [
    "Plot the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21f2c5-0bab-4fa6-9dc0-dca8118c6010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72dbc6fc-fb75-43b7-8de2-2a63d3f2d110",
   "metadata": {},
   "source": [
    "The posterior samples show us that the slop of the function is positive, so $\\theta$ becomes larger for higher registers.\n",
    "But what exactly does that mean?\n",
    "Let's first look at the resulting sigmoid shape of $\\theta$ by using the expected values of $a$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a243c-60ef-4752-bade-b2576ae592c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sigmoid(a, b):\n",
    "    \"This function plots a sigmoid curve for a given pair of model parameters a and b.\"\n",
    "    x = np.arange(-100,250)\n",
    "    # y = 1 / (1 + np.exp(-(a*x + b)))\n",
    "    y = pm.math.sigmoid(x*a + b).eval()\n",
    "    return so.Plot(x=x, y=y).add(so.Line()).add(so.Area(), x=x[130:200], y=y[130:200])\n",
    "\n",
    "plot_sigmoid(idata_register.posterior[\"a\"].mean().item(), idata_register.posterior[\"b\"].mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2bd0b9-79d4-4291-b723-27561b89b285",
   "metadata": {},
   "source": [
    "So $\\theta$ changes quite a bit from less than 0.2 for low pitches to almost 0.6 for the highest.\n",
    "But we can't really see the uncertainty here, and it's still a bit difficult to imagine what that means for the intervals.\n",
    "So let's look at the *expected interval size* in different registers,\n",
    "and actually use the posterior samples to get an idea of how much this could vary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe493b1-044d-4beb-b77b-5c7af0bee5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_sizes(a, b):\n",
    "    \"\"\"\n",
    "    This function plots curves of the expected interval size given the preceding pitch\n",
    "    for several pairs of model parameters.\n",
    "    `a` contains all values for a, `b` contains all parameters for b,\n",
    "    as found in the inference data.\n",
    "    Also plots a curve for the mean value and highlights the range of pitches in the dataset.\n",
    "    \"\"\"\n",
    "    # select some samples\n",
    "    ny = 1_000\n",
    "    indices = rng.choice(np.arange(a.size), ny, replace=False)\n",
    "\n",
    "    # compute the sigmoids for the selected samples\n",
    "    x = np.arange(0,120)\n",
    "    y = (1 + np.exp(-(np.atleast_2d(a[indices]).T * np.atleast_2d(x) + np.atleast_2d(b[indices]).T)))\n",
    "\n",
    "    # create a dataframe that holds the sample sigmoids, this makes it easier to plot with seaborn\n",
    "    data = pd.DataFrame({\n",
    "        \"pitch\": np.tile(x, ny),\n",
    "        \"E[int]\": y.flatten(),\n",
    "        \"group\": np.concatenate([np.full(x.size, iy) for iy in range(ny)])\n",
    "    })\n",
    "    \n",
    "    # compute the sigmoid for the sample mean over all samples (same as previous plot)\n",
    "    amean = a.mean()\n",
    "    bmean = b.mean()\n",
    "    ymean = (1 + np.exp(-(amean*x + bmean)))\n",
    "\n",
    "    # create the plot with sample sigmoids and mean sigmoid\n",
    "    return (\n",
    "        so.Plot()\n",
    "            .add(so.Line(alpha=0.01, color=\".2\"), data=data, x=\"pitch\", y=\"E[int]\", group=\"group\")\n",
    "            .add(so.Line(color=\"g\"), x=x, y=ymean)\n",
    "            .add(so.Area(color=\"g\"), x=x[30:100], y=ymean[30:100])\n",
    "    )\n",
    "\n",
    "plot_expected_sizes(idata_register.posterior[\"a\"].to_numpy().flatten(), idata_register.posterior[\"b\"].to_numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68cd9d-f7e8-4ade-a1da-28c0834434f4",
   "metadata": {},
   "source": [
    "One thing that we can observe here is that the uncertainty is smallest in the center of our pitch range\n",
    "because that is where we have most observations.\n",
    "We don't have any observations outside the highlighted range, so it's only natural that the uncertainty increases there.\n",
    "\n",
    "Another thing that follows from this shape is that $a$ and $b$ probably don't vary independently.\n",
    "If $a$ is a bit flatter or steeper, then $b$ is adjusted accordingly to still go throught the middle of the data.\n",
    "We can actually see this correlation if we plot the corresponding samples of $a$ and $b$ together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088b2fc-a79d-4ea1-91ea-828f7bd16b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "so.Plot(x=idata_register.posterior[\"a\"].to_numpy().flatten(), y=idata_register.posterior[\"b\"].to_numpy().flatten()).add(so.Dots())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fac14-fe72-4a29-9196-7c3cb06ac2da",
   "metadata": {},
   "source": [
    "# Checking the model: simulate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd0fba-5f1d-4152-a44f-4310cfcbb42a",
   "metadata": {},
   "source": [
    "So how good (or bad) are these models really?\n",
    "There are several ways to check this but one very intuitive way is to do a \"simulation\"\n",
    "and generate new data with our models.\n",
    "If the models tell a good generative story about the dataset,\n",
    "then they should also generate new data that looks somewhat like our observations.\n",
    "With probabilistic programs, it is particularly easy to do this since they already have a generative form.\n",
    "And probabilistic programming libraries provide special tools for sampling from the *posterior predictive* distribution,\n",
    "i.e. the distribution of *new data* after observing old data:\n",
    "\n",
    "$p(x' | \\vec{x})$\n",
    "\n",
    "In PyMC, we do this by taking our sample of posterior parameter values in the inference data.\n",
    "For each set of parameter values in this sample, a new set of \"observation\" ($\\vec{x'}$)\n",
    "is sampled from the posterior predictive distribution.\n",
    "Since our observations are already long arrays, we would normally get many more observations in total then we have originally\n",
    "namely `n_obs * n_samples`.\n",
    "However, we can also choose to generate fewer new points for each posterior sample, e.g. just one new $x'$ for each set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28049b7a-ddff-4221-9a67-2894256e700c",
   "metadata": {},
   "source": [
    "## By Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55e49e-13dd-40ae-b4b9-701b9ff5a548",
   "metadata": {},
   "source": [
    "We can reuse our per-voice model,\n",
    "but in order to avoid sampling too many new observations,\n",
    "we replace the data that is used in the model,\n",
    "i.e., the observed intervals and the voices.\n",
    "\n",
    "So far, the data is hardcoded in the model and can't be changed.\n",
    "There are two ways to get around that:\n",
    "Either create a second model, identcal to the first one but with different data,\n",
    "or we change the model to use `pm.MutableData()` containers.\n",
    "These containers are very simple: in the model definition,\n",
    "you create a `MutableData` object that you give a name and the inital data that you want to use.\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613beeeb-dd80-459d-b295-88b2978f5b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as example_model:\n",
    "    # initialize the mutable data container\n",
    "    some_data = pm.MutableData(\"name_of_data\", [1,2,3,4,5])\n",
    "    # use the data in the rest of the model, just like before\n",
    "    pm.Binomial(\"obs\", 10, 0.5, observed=some_data)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579afe8-ec3a-47e1-bd71-93c8a1149e12",
   "metadata": {},
   "source": [
    "In a second step, you can go back \"into\" the model using\n",
    "```python\n",
    "with example_model:\n",
    "   ...\n",
    "```\n",
    "and then replace the data in the container using `pm.set_data()`.\n",
    "This function takes a dictionary (`{\"key\": value, ...}`),\n",
    "where the keys are the names for the data containers that you define in `pm.MutableData()`,\n",
    "and the values are the new values for the respective containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f61598-d373-4a2c-af20-c5923529fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with example_model:\n",
    "    pm.set_data({\"name_of_data\": [6,7,8]})\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2ef47-118f-4c1c-b433-eebc4e1714c1",
   "metadata": {},
   "source": [
    "Rewrite the voice model from above to use a `MutableData` container\n",
    "for both the observed intervals and the corresponding voices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87d94a-490b-46ad-8fa8-c1f1e519eded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "574993e0-1495-41e8-9be1-086ac5aaecba",
   "metadata": {},
   "source": [
    "Now, let's modify the data in the original model so that we can use it for simulation.\n",
    "Remember that sampling from the posterior already gave us a few thousand sets of parameter values.\n",
    "For simulating new outputs of the model given these parameters,\n",
    "it should be sufficient to only take each parameter set, and sample a few intervals per voice,\n",
    "or maybe even just one.\n",
    "To achieve this, we change the data inside the model.\n",
    "Instead of huge data arrays with many examples of intervals in each voice,\n",
    "we only use one entry per voice, so the `voice` array becomes `[0,1,2,3]`.\n",
    "For `obs`, we just provide a dummy array like `[1,1,1,1]`\n",
    "since we resample its values anyways.\n",
    "(These values will be ignored, but we need to provide something to make the model happy.)\n",
    "If we do this, we will get new intervals, one for each voice and for each set of parameters in the posterior sample.\n",
    "\n",
    "Go back into your new version of the voice model (using `with`) and\n",
    "- set the model data to the new values using `pm.set_data`;\n",
    "- use `pm.sample_posterior_predictive(idata_voice, extend_inferencedata=True)` to generate new intervals.\n",
    "\n",
    "You have to give the function `pm.sample_posterior_predictive` your inference data object that contains the posterior sample.\n",
    "Adding the keyword `extend_inferencedata=True` will add the new samples to the old idata object instead of creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a113b8-ec90-4cfc-9eab-5c4611fd7935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e415c4-d431-4d4e-a33f-f8f3222ff8fd",
   "metadata": {},
   "source": [
    "Run the cell below to plot the simulation results compared to the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6a3e0-d667-4ed6-8cd5-ebdd29d087ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_voice(idata, original):\n",
    "    \"\"\"\n",
    "    This function takes an idata object containing posterior predictive samples,\n",
    "    as well as the dataframe containing the original data.\n",
    "\n",
    "    It will plot histograms with the interval sizes for each voice and group (original vs generated).\n",
    "    \"\"\"\n",
    "    gen_samples = idata.posterior_predictive[\"obs\"].to_numpy().flatten()\n",
    "    n_chains = idata.posterior_predictive.dims[\"chain\"]\n",
    "    n_draws = idata.posterior_predictive.dims[\"draw\"]\n",
    "    gen_df = pd.DataFrame({\n",
    "        \"staff\": np.tile([1,2,3,4], n_chains * n_draws),\n",
    "        \"dist_semitones\": gen_samples-1,\n",
    "        #\"bigram_id\": np.repeat(np.arange(n_chains * n_draws), 4)\n",
    "    })\n",
    "    df_voice = pd.concat((original.reset_index(), gen_df),\n",
    "                         join=\"inner\",\n",
    "                         keys=[\"observed\", \"generated\"],\n",
    "                         names=[\"source\", \"sample\"])\n",
    "\n",
    "    return (\n",
    "        so.Plot(df_voice, x=\"dist_semitones\")\n",
    "            .facet(row=\"source\", col=\"staff\")\n",
    "            .add(so.Bars(alpha=0.5), so.Hist(\"density\", discrete=True))\n",
    "            #.add(so.Line(), so.KDE())\n",
    "            .layout(size=(13,5))\n",
    "    )\n",
    "\n",
    "plot_compare_voice(idata_voice, dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd8b1c-2479-489b-b509-7feb2882a8a4",
   "metadata": {},
   "source": [
    "We can observe that the different $\\theta$s for the different voices also correspond to different widths in the observed distributions,\n",
    "so the model is able to reflect this aspect of the data.\n",
    "On the other hand, there are still big differences between the observed distributions and the inferred ones.\n",
    "For example, $0$ (unison) is usually way to common in the model compared to steps (1 or 2 semitones),\n",
    "which is a problem of the geometric distribution.\n",
    "Another aspect is that there seem to be \"systematic outliers\", e.g., some voices have little bumps at 5 (4th), 7 (5th) and 12 (octave).\n",
    "A better model would probably take the special functions of steps as well as perfect intervals into account!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb32d2-028a-42c0-9d81-0de0928e7469",
   "metadata": {},
   "source": [
    "## By Register"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cb769-fabc-4570-9c3a-56450c2c4289",
   "metadata": {},
   "source": [
    "For the register model, we use the same procedure as above,\n",
    "but this time we draw one observation for each possible pitch $p_0$ at each posterior sample instead of each voice.\n",
    "We compute the range of preceding pitches in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2644794-da7a-41a3-bbb8-2d10fbe04cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0_range = np.arange(p0.min(), p0.max()+1)\n",
    "fake_obs_p0 = np.ones(p0_range.shape, dtype=int) # these are the fake observations\n",
    "\n",
    "p0_range, fake_obs_p0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45f100-6532-47e4-864f-70211d1b5596",
   "metadata": {},
   "source": [
    "First, rewrite the register model to use `MutableData` containers for `p0` and `obs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e02b4-d6cf-4110-bf62-5f3a678b10ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2980b668-1413-44c7-9212-d02ef6ec76be",
   "metadata": {},
   "source": [
    "Then, use this model, replace the original data with `p0_range` and `fake_obs_p0` (all 1s)\n",
    "and sample new observations as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d63c8-be28-45e8-a5a7-d28d0dc96c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "492abf3d-e6f5-4c49-bd97-8864f2ba9280",
   "metadata": {},
   "source": [
    "Again, let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06b16a-a509-4d12-aec4-90b72b9c094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_vs_generated(idata, original):\n",
    "    \"\"\"\n",
    "    Plots histograms comparing simulated data from the register model\n",
    "    to the original data.\n",
    "    \"\"\"\n",
    "    gen_register = idata.posterior_predictive[\"obs\"].to_numpy().flatten()\n",
    "    n_chains = idata.posterior_predictive.dims[\"chain\"]\n",
    "    n_draws = idata.posterior_predictive.dims[\"draw\"]\n",
    "    gen_df = pd.DataFrame({\n",
    "        \"n0_midi\": np.tile(p0_range, n_chains*n_draws),\n",
    "        \"dist_semitones\": gen_register.flatten()-1,\n",
    "        #\"sample\": np.repeat(np.arange(n_chains*n_draws), len(p0_range))\n",
    "    })\n",
    "    # combine dataframes, downsample generated to common size\n",
    "    subsample = rng.choice(np.arange(len(p0_range)*n_chains*n_draws), len(original), replace=False)\n",
    "    df = pd.concat([original.reset_index(), gen_df.iloc[subsample]], join=\"inner\", keys=[\"observed\", \"generated\"], names=[\"source\", \"sample\"])\n",
    "\n",
    "    def cond_hist(data):\n",
    "        p0 = data.n0_midi\n",
    "        dist = data.dist_semitones\n",
    "        xmin, xmax = p0.min(), p0.max()\n",
    "        ymin, ymax = dist.min(), min(24, dist.max())\n",
    "        \n",
    "        hist, xedges, yedges = np.histogram2d(p0, dist, range=[[xmin-0.5, xmax+0.5], [ymin-0.5, ymax+0.5]], bins=(xmax-xmin+1, ymax-ymin+1))\n",
    "        marg = hist.sum(axis=1, keepdims=True)\n",
    "        marg[marg==0] = 1\n",
    "        hist_cond = hist / marg\n",
    "        return pd.DataFrame(data=hist_cond.T, columns=np.arange(xmin, xmax+1), index=np.arange(ymin, ymax+1))\n",
    "        \n",
    "    def plot_hist(ax, df, vmax, cbar=True):\n",
    "        sns.heatmap(df, ax=ax, cmap=\"Blues\", vmin=0, vmax=vmax, cbar=cbar)\n",
    "\n",
    "    hist_orig = cond_hist(original)\n",
    "    hist_gen = cond_hist(gen_df)\n",
    "    vmax = max(hist_orig.max(axis=None), hist_gen.max(axis=None))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13,4), sharex=True, sharey=True)\n",
    "    plot_hist(ax[0], hist_orig, vmax)\n",
    "    plot_hist(ax[1], hist_gen, vmax)\n",
    "    ax[0].invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    # return fig\n",
    "    return so.Plot(df, x=\"n0_midi\", y=\"dist_semitones\").facet(\"source\").add(so.Dots()).layout(size=(13,5))\n",
    "\n",
    "plot_data_vs_generated(idata_register, dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d02f19-3b97-4560-9f15-db340d682140",
   "metadata": {},
   "source": [
    "We can make similar observations as above. The general tendency seems to be right,\n",
    "but the model still doesn't capture a lot of the structure in the data, e.g. the bands at 5, 7, and 12.\n",
    "In addition, there seems to be some \"checkerboard\" structure in the steps:\n",
    "some pitches are followed by halftone steps, others by wholetones.\n",
    "This could be caused by a bias in the keys that Bach uses, e.g. a preference for stem tones.\n",
    "Generally, the original distribution is much more focussed on particular interval sizes than the relatively smooth model distribution.\n",
    "\n",
    "(Note that the upper plot is a bit misleading about the variance of the given vs. the generated data.\n",
    "We still have many more generated samples than given observations, so it's much more likely to sample larger intervals,\n",
    "and the scatter plot isn't very good at showing that.\n",
    "It's more apparent in the heatmap plot below, which is normalized by column, so each column shows the distribution $p(x|p_0)$.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmth-bayes-tutorial",
   "language": "python",
   "name": "gmth-bayes-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
